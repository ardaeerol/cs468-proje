{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6b3e2c6",
   "metadata": {},
   "source": [
    "# Image Process and Feature Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee79a928",
   "metadata": {},
   "source": [
    "Dataset in use: https://susanqq.github.io/UTKFace/\n",
    "\n",
    "*In-the-wild Faces is used and part-2 is selected for train, part-3 is selected for test set.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40bfc650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn import metrics\n",
    "from skimage import feature\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "885d7560",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "375f1b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir(path):\n",
    "    try:\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "            logging.debug(f\"Folder '{path}' created successfully.\")\n",
    "        else:\n",
    "            raise RuntimeError(f\"Folder '{path}' already exists.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error creating folder '{path}': {e} You are going to modify an existing file\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9e0593b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Raporda belirtildigi gibi, girdi olarak net cekilen 1 adet yuz goruntusu verilir ve cikti olarak da gender saglanir. Bu nedenle; \n",
    "\n",
    "SIFT, SURF, distance vb. için tabi ki hazır fonksiyonları kullanacaksınız ama göz bulma, yüz bulma vb. işler için \n",
    "hazır kütüphane kullanamazsınız.\n",
    "\n",
    "ibaresi proje ile alakali degildir. Projemiz yuz tespiti degil, girdi olarak verilen yuz goruntusunden gender tespitidir. \n",
    "Girdi hatasi olmasi durumunda kodun yine de calismasi amaciyla hazir kutuphane kullanilmaktadir.\n",
    "'''\n",
    "\n",
    "def face_extractor(source_dir, dest_dir, ext='jpg'):\n",
    "    mkdir(dest_dir)\n",
    "    haar_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml') \n",
    "    \n",
    "    pimgs = glob.glob(f\"{source_dir}/*.{ext}\")\n",
    "    \n",
    "    for pimg in tqdm(pimgs, desc=\"Cropping faces from wild images\"):\n",
    "        img=cv2.imread(pimg)\n",
    "        gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) \n",
    "        faces_rect = haar_cascade.detectMultiScale(gray_img, 1.2, 5) \n",
    "        \n",
    "        if faces_rect is ():\n",
    "            logging.debug('No face detected')\n",
    "            continue\n",
    "        \n",
    "        for (x, y, w, h) in faces_rect: \n",
    "            cv2.rectangle(img, (x, y), (x+w, y+h), (0, 255, 0), 2) \n",
    "            cropped_face = gray_img[y:y+h, x:x+w]\n",
    "            img_name = os.path.splitext(os.path.basename(pimg))[0]\n",
    "            cv2.imwrite(f'{dest_dir}/{img_name}_cropped.jpg', cropped_face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54b18e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_report(actual, predicted):\n",
    "    acc = metrics.accuracy_score(actual, predicted)\n",
    "    precision = metrics.precision_score(actual, predicted)\n",
    "    recall = metrics.recall_score(actual, predicted)\n",
    "    f1 = metrics.f1_score(actual, predicted)    \n",
    "    return (acc, precision, recall, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "554b56bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir='./utk_train'\n",
    "dest_dir='./utk_train_cropped'\n",
    "ext='jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35e13c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_source_dir='./utk_test'\n",
    "test_dest_dir='./utk_test_cropped'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7057bc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = './test_images'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd834f1f",
   "metadata": {},
   "source": [
    "## Local Binary Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb6b53d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalBinaryPatterns:\n",
    "    def __init__(self, numPoints, radius):\n",
    "        self.numPoints = numPoints\n",
    "        self.radius = radius\n",
    "\n",
    "    def describe(self, image, eps=1e-7):\n",
    "        lbp = feature.local_binary_pattern(image, self.numPoints, self.radius, method=\"uniform\")\n",
    "        (hist, _) = np.histogram(lbp.ravel(),\n",
    "                                 bins=np.arange(0, self.numPoints + 3),\n",
    "                                 range=(0, self.numPoints + 2))\n",
    "        \n",
    "        hist = hist.astype(\"float\")\n",
    "        hist /= (hist.sum() + eps)\n",
    "        return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2742c677",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_image_pipeline(img, desc, male_hist_path, female_hist_path):\n",
    "    fimg = cv2.imread(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    lbp_hist = desc.describe(fimg)\n",
    "    lbp_hist = lbp_hist.astype(np.float32)\n",
    "\n",
    "    male_lbp_hist = np.load(male_hist_path).astype(np.float32)\n",
    "    female_lbp_hist = np.load(female_hist_path).astype(np.float32)\n",
    "\n",
    "    male_distance = cv2.compareHist(lbp_hist, male_lbp_hist, cv2.HISTCMP_INTERSECT)\n",
    "    female_distance = cv2.compareHist(lbp_hist, female_lbp_hist, cv2.HISTCMP_INTERSECT)    \n",
    "    # HISTCMP_INTERSECT, HISTCMP_CORREL, HISTCMP_BHATTACHARYYA, HISTCMP_HELLINGER, HISTCMP_CHISQR \n",
    "    \n",
    "    return 0 if male_distance >= female_distance else 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f05e5cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nUTK Train setindeki goruntulerden yuzler elde edilir ve dest_dir uzerine kaydedilir.\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "UTK Train setindeki goruntulerden yuzler elde edilir ve dest_dir uzerine kaydedilir.\n",
    "'''\n",
    "#face_extractor(source_dir, dest_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff86dd2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmale_images=[]\\nfemale_images=[]\\nfor pimg in tqdm(glob.glob(f\"{dest_dir}/*.{ext}\"), desc=\"Creating Male and Female Arrays\"):\\n    img = cv2.imread(pimg, cv2.COLOR_BGR2GRAY)\\n    female_images.append(img) if int(os.path.basename(pimg).split(\\'_\\')[1]) else male_images.append(img)\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Elde edilen yuz goruntuleri cinsiyete gore ayristirilir.\n",
    "'''\n",
    "'''\n",
    "male_images=[]\n",
    "female_images=[]\n",
    "for pimg in tqdm(glob.glob(f\"{dest_dir}/*.{ext}\"), desc=\"Creating Male and Female Arrays\"):\n",
    "    img = cv2.imread(pimg, cv2.COLOR_BGR2GRAY)\n",
    "    female_images.append(img) if int(os.path.basename(pimg).split('_')[1]) else male_images.append(img)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9baf292f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f'Number of male images in trainset: {len(male_images)}\\nNumber of female images in trainset: {len(female_images)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e336900d",
   "metadata": {},
   "outputs": [],
   "source": [
    "desc=LocalBinaryPatterns(24, 8) # define descriptor instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e51667b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmale_histograms = [desc.describe(img) for img in tqdm(male_images, desc=\"Computing LBP histogram - Male\")]\\nfemale_histograms = [desc.describe(img) for img in tqdm(female_images, desc=\"Computing LBP histogram - Female\")]\\n\\nmale_lbp_hist = np.mean(male_histograms, axis=0)\\nfemale_lbp_hist = np.mean(female_histograms, axis=0)\\n\\nnp.save(\\'male_lbp_hist.npy\\', male_lbp_hist)\\nnp.save(\\'female_lbp_hist.npy\\', female_lbp_hist)\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Male-Female LBP histogramlari elde edilir ve kaydedilir. \n",
    "'''\n",
    "'''\n",
    "male_histograms = [desc.describe(img) for img in tqdm(male_images, desc=\"Computing LBP histogram - Male\")]\n",
    "female_histograms = [desc.describe(img) for img in tqdm(female_images, desc=\"Computing LBP histogram - Female\")]\n",
    "\n",
    "male_lbp_hist = np.mean(male_histograms, axis=0)\n",
    "female_lbp_hist = np.mean(female_histograms, axis=0)\n",
    "\n",
    "np.save('male_lbp_hist.npy', male_lbp_hist)\n",
    "np.save('female_lbp_hist.npy', female_lbp_hist)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6cc3ed7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#face_extractor(source_dir, dest_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0956b5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Test veri setinden rastgele goruntu secilerek cinsiyet tahmini yapilir.\n",
    "'''\n",
    "\n",
    "random_image_pick = np.random.choice(glob.glob(test_dir + f'/*.{ext}'))\n",
    "pred = single_image_pipeline(random_image_pick, desc, 'male_lbp_hist.npy', 'female_lbp_hist.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "469472d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntimgs = glob.glob(f\"{test_dest_dir}/*.{ext}\") #test images\\n\\nactual, predicted = [], []\\nfor timg in tqdm(timgs, desc=\"Inference in progress\"):\\n    gender = single_image_pipeline(timg, desc, \\'male_lbp_hist.npy\\', \\'female_lbp_hist.npy\\')  \\n    label = int(os.path.basename(timg).split(\\'_\\')[1])\\n    \\n    actual.append(label)\\n    predicted.append(gender)\\n    \\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Tum test verisi uzerinde tahminler gerceklestirilir.\n",
    "'''\n",
    "\n",
    "'''\n",
    "timgs = glob.glob(f\"{test_dest_dir}/*.{ext}\") #test images\n",
    "\n",
    "actual, predicted = [], []\n",
    "for timg in tqdm(timgs, desc=\"Inference in progress\"):\n",
    "    gender = single_image_pipeline(timg, desc, 'male_lbp_hist.npy', 'female_lbp_hist.npy')  \n",
    "    label = int(os.path.basename(timg).split('_')[1])\n",
    "    \n",
    "    actual.append(label)\n",
    "    predicted.append(gender)\n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5023c04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#metric_report(actual, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "23682e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion_matrix = metrics.confusion_matrix(actual, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6a6cef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = ['Male', 'Female'])\n",
    "#cm_display.plot()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94383159",
   "metadata": {},
   "outputs": [],
   "source": [
    "#female_data = np.sum(actual); male_data = len(actual) - female_data\n",
    "#female_data, male_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d375c960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gender Detected!\n"
     ]
    }
   ],
   "source": [
    "# Asagidaki img_path degiskenine istenen goruntulerin pathi verilir ve lbp ile gender detect edilir.\n",
    "\n",
    "img_path = 'test_images/22_0_0_20170119151204830_cropped.jpg'\n",
    "actual = int(os.path.basename(img_path).split('_')[1])\n",
    "\n",
    "pred = single_image_pipeline(img_path, desc, 'male_lbp_hist.npy', 'female_lbp_hist.npy')\n",
    "\n",
    "if actual == pred:\n",
    "    print('Gender Detected!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c01ca76",
   "metadata": {},
   "source": [
    "## Scale Invariant Feature Transform (SIFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "95f92db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sift = cv2.SIFT_create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aee5c197",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flann_match(query_descriptors, train_descriptors):\n",
    "    FLANN_INDEX_KDTREE = 1\n",
    "    index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "    search_params = dict(checks=50)\n",
    "\n",
    "    flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "    matches = flann.knnMatch(query_descriptors, np.vstack(train_descriptors), k=2)\n",
    "\n",
    "    # Ratio test as per Lowe's paper\n",
    "    good_matches = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < 0.7 * n.distance:\n",
    "            good_matches.append(m)\n",
    "    return len(good_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9909766b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_descriptors(image, extractor):\n",
    "    gray = cv2.imread(image, 0)   \n",
    "    keypoints, descriptors = extractor.detectAndCompute(gray, None)\n",
    "    return descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "74ba0d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_image_pipeline_sift(img, male_desc_path, female_desc_path):\n",
    "    descriptor = extract_descriptors(img, sift)\n",
    "\n",
    "    male_desc = np.load(male_desc_path).astype(np.float32)\n",
    "    female_desc = np.load(female_desc_path).astype(np.float32)\n",
    "\n",
    "    male_distance = flann_match(descriptor, male_desc)\n",
    "    female_distance = flann_match(descriptor, female_desc)   \n",
    "    \n",
    "    return 0 if male_distance >= female_distance else 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "694b0f71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmale_descriptors = []\\nfemale_descriptors = []\\n\\nfor img in tqdm(male_images, desc= 'Extracting descriptors - Male'):\\n    desc = extract_descriptors(img)\\n    if desc is not None:\\n        male_descriptors.extend(desc)\\n\\nfor img in tqdm(female_images, desc='Extracting descriptors - Female'):\\n    desc = extract_descriptors(img)\\n    if desc is not None:\\n        female_descriptors.extend(desc)\\n\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Male-Female SIFT descriptor elde edilir. \n",
    "'''\n",
    "\n",
    "'''\n",
    "male_descriptors = []\n",
    "female_descriptors = []\n",
    "\n",
    "for img in tqdm(male_images, desc= 'Extracting descriptors - Male'):\n",
    "    desc = extract_descriptors(img)\n",
    "    if desc is not None:\n",
    "        male_descriptors.extend(desc)\n",
    "\n",
    "for img in tqdm(female_images, desc='Extracting descriptors - Female'):\n",
    "    desc = extract_descriptors(img)\n",
    "    if desc is not None:\n",
    "        female_descriptors.extend(desc)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1e659fe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmale_descriptors = np.array(male_descriptors, dtype=np.float32)\\nfemale_descriptors = np.array(female_descriptors, dtype=np.float32)\\n\\nnp.save('male_descriptors.npy', male_descriptors)\\nnp.save('female_descriptors.npy', female_descriptors)\\n\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Cikartilan descriptorlar kaydedilir.\n",
    "'''\n",
    "\n",
    "'''\n",
    "male_descriptors = np.array(male_descriptors, dtype=np.float32)\n",
    "female_descriptors = np.array(female_descriptors, dtype=np.float32)\n",
    "\n",
    "np.save('male_descriptors.npy', male_descriptors)\n",
    "np.save('female_descriptors.npy', female_descriptors)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bd8b29bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "male_descriptors = np.load('male_descriptors.npy')\n",
    "female_descriptors = np.load('female_descriptors.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fa33a719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./utk_test_cropped/'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'./utk_test_cropped/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3c58c97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SIFT Inference: 100%|████████████████████████████████████████████████████████| 10/10 [02:18<00:00, 13.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Gender: Male, Actual Gender: Male\n",
      "Predicted Gender: Male, Actual Gender: Female\n",
      "Predicted Gender: Female, Actual Gender: Female\n",
      "Predicted Gender: Female, Actual Gender: Male\n",
      "Predicted Gender: Female, Actual Gender: Female\n",
      "Predicted Gender: Male, Actual Gender: Male\n",
      "Predicted Gender: Male, Actual Gender: Male\n",
      "Predicted Gender: Female, Actual Gender: Female\n",
      "Predicted Gender: Female, Actual Gender: Female\n",
      "Predicted Gender: Male, Actual Gender: Male\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results=[]\n",
    "for _ in tqdm(range(10), desc='SIFT Inference'):\n",
    "    random_image_picked = np.random.choice(glob.glob(test_dir + f'/*.{ext}'), replace=False)\n",
    "    img = cv2.imread(random_image_picked, 0)\n",
    "    keypoints, descriptors = sift.detectAndCompute(img, None)\n",
    "\n",
    "    male_matches = flann_match(descriptors, male_descriptors)\n",
    "    female_matches = flann_match(descriptors, female_descriptors)\n",
    "\n",
    "    predicted_gender = 0 if male_matches > female_matches else 1\n",
    "    actual_gender = int(os.path.basename(random_image_picked).split('_')[1])  # Assumes filename format includes gender as second element after splitting by '_'\n",
    "    \n",
    "    results.append((predicted_gender, actual_gender))\n",
    "\n",
    "for result in results:\n",
    "    print(f\"Predicted Gender: {'Male' if result[0] == 0 else 'Female'}, Actual Gender: {'Male' if result[1] == 0 else 'Female'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ffe808",
   "metadata": {},
   "source": [
    "Compared to LBP-H algorithm implemented above, SIFT also performed same accuracy on the benchmark tests. Because of computational problems in FLANN, metrics cannot be obtained directly but sampling is used.<br>\n",
    "\n",
    "SIFT has approximately %70 (Accuracy) on the UTK Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "96ceaf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asagidaki img_path degiskenine istenen goruntulerin pathi verilir ve sift ile gender detect edilir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "351df155",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = 'test_images/16_0_0_20170120133327900_cropped.jpg'\n",
    "actual = int(os.path.basename(img_path).split('_')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7b57a742",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = single_image_pipeline_sift(img_path, 'male_descriptors.npy', 'female_descriptors.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "de940b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gender Detected!\n"
     ]
    }
   ],
   "source": [
    "if actual == pred:\n",
    "    print('Gender Detected!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e350a30f",
   "metadata": {},
   "source": [
    "Flann yerine baska bir algoritma ile daha hizli sonuclar dondurulecektir. Akabinde metrikler de gosterilecektir. Kullanilacak bazi algoritmalar 21 Mart dersinde islenmistir ve hizla koda eklenecektir."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
